{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Question 3</h1>\n",
    "<br>In Spark ML given the hard drive logs for 2019 Q1, implement a point anomaly detector for: \n",
    "<br>a) Annualized Failure Rate (by model) \n",
    "<br>b) Normalized Read Error Rate, SMART attribute 1. \n",
    "<br>- For generating training labels, use        a) 2%           b) 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Question 3(a):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting the spark session\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from scipy.spatial import distance\n",
    "\n",
    "#starting the spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data from different csv files to a single csv and making a datarame\n",
    "pre_final= spark.read.format(\"csv\").option(\"header\",\"true\").load(r\"C:\\Users\\myste\\Documents\\MSCS\\Spring 2020\\Big Data\\data_Q2_SP20\\*.csv\")\n",
    "pre_final.createOrReplaceTempView(\"drive_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               model|drive_days|\n",
      "+--------------------+----------+\n",
      "|         ST4000DM000|   1989429|\n",
      "|       ST12000NM0007|   2955025|\n",
      "|         ST8000DM005|      2250|\n",
      "|          ST320LT007|        85|\n",
      "| TOSHIBA MQ01ABF050M|     32624|\n",
      "|        ST8000NM0055|   1294451|\n",
      "|Seagate BarraCuda...|       265|\n",
      "| TOSHIBA MG07ACA14TA|    109404|\n",
      "|        WDC WD60EFRX|     30523|\n",
      "|         ST8000DM002|    888741|\n",
      "|         ST4000DM005|      4848|\n",
      "|         DELLBOSS VD|       540|\n",
      "|HGST HUS726040ALE610|      2598|\n",
      "|     TOSHIBA HDWF180|      1798|\n",
      "|HGST HMS5C4040ALE640|    313383|\n",
      "|HGST HUH721010ALE600|      1245|\n",
      "| TOSHIBA MD04ABA500V|      4050|\n",
      "| TOSHIBA MD04ABA400V|     12662|\n",
      "|       ST10000NM0086|    108555|\n",
      "|      WDC WD2500AAJS|        88|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating a table for drive days\n",
    "drive_days_df=spark.sql(\"SELECT model, count(*) AS drive_days FROM drive_stats GROUP BY model\")\n",
    "drive_days_df.createOrReplaceTempView(\"drive_days\")\n",
    "drive_days_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|               model|failures|\n",
      "+--------------------+--------+\n",
      "|         ST4000DM000|     107|\n",
      "|       ST12000NM0007|     180|\n",
      "| TOSHIBA MQ01ABF050M|       3|\n",
      "|        ST8000NM0055|      58|\n",
      "|        WDC WD60EFRX|       1|\n",
      "| TOSHIBA MG07ACA14TA|       1|\n",
      "|         ST8000DM002|      29|\n",
      "|HGST HMS5C4040ALE640|       2|\n",
      "|       ST10000NM0086|       3|\n",
      "|  TOSHIBA MQ01ABF050|      14|\n",
      "|HGST HMS5C4040BLE640|      11|\n",
      "|         ST6000DX000|       1|\n",
      "|      WDC WD5000LPVX|       2|\n",
      "|          ST500LM030|       9|\n",
      "|       ST500LM012 HN|      12|\n",
      "|HGST HUH728080ALE600|       3|\n",
      "|HGST HUH721212ALE600|       1|\n",
      "|      WDC WD5000LPCX|       2|\n",
      "|         ST8000DM004|       1|\n",
      "|HGST HUH721212ALN604|       4|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating a table for hard drive failures\n",
    "failures_df=spark.sql(\"SELECT model, count(*) AS failures FROM drive_stats WHERE failure = 1 GROUP BY model\")\n",
    "failures_df.createOrReplaceTempView(\"failures\")\n",
    "failures_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               model|count|\n",
      "+--------------------+-----+\n",
      "|         ST4000DM000|19785|\n",
      "|       ST12000NM0007|34708|\n",
      "|         ST8000DM005|   25|\n",
      "| TOSHIBA MQ01ABF050M|  377|\n",
      "|        ST8000NM0055|14381|\n",
      "|Seagate BarraCuda...|    3|\n",
      "| TOSHIBA MG07ACA14TA| 1220|\n",
      "|        WDC WD60EFRX|   89|\n",
      "|         ST8000DM002| 9874|\n",
      "|         ST4000DM005|   43|\n",
      "|         DELLBOSS VD|   20|\n",
      "|HGST HUS726040ALE610|   28|\n",
      "|     TOSHIBA HDWF180|   20|\n",
      "|HGST HMS5C4040ALE640| 2557|\n",
      "|HGST HUH721010ALE600|   17|\n",
      "| TOSHIBA MD04ABA500V|   45|\n",
      "| TOSHIBA MD04ABA400V|   99|\n",
      "|       ST10000NM0086| 1203|\n",
      "|  TOSHIBA MQ01ABF050|  515|\n",
      "|          ST500LM021|   14|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating a table for model count\n",
    "model_count_df=spark.sql(\"SELECT model, count(*) AS count FROM drive_stats WHERE date = '2019-03-31' GROUP BY model\")\n",
    "model_count_df.createOrReplaceTempView(\"model_count\")\n",
    "model_count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+-------------------+\n",
      "|               model|drive_days|failures|annual_failure_rate|\n",
      "+--------------------+----------+--------+-------------------+\n",
      "|HGST HMS5C4040ALE640|    313383|       2|           0.232942|\n",
      "|HGST HMS5C4040BLE640|   1172824|      11|           0.342336|\n",
      "|HGST HUH721212ALE600|     14040|       1|           2.599715|\n",
      "|HGST HUH721212ALN604|    259749|       4|           0.562081|\n",
      "|HGST HUH728080ALE600|     93598|       3|           1.169897|\n",
      "|       ST10000NM0086|    108555|       3|           1.008705|\n",
      "|       ST12000NM0007|   2955025|     180|           2.223331|\n",
      "|         ST4000DM000|   1989429|     107|           1.963126|\n",
      "|       ST500LM012 HN|     50619|      12|           8.652877|\n",
      "|          ST500LM030|     14479|       9|          22.688031|\n",
      "|         ST6000DX000|    135832|       1|           0.268714|\n",
      "|         ST8000DM002|    888741|      29|           1.191011|\n",
      "|         ST8000DM004|       273|       1|         133.699670|\n",
      "|        ST8000NM0055|   1294451|      58|           1.635442|\n",
      "| TOSHIBA MG07ACA14TA|    109404|       1|           0.333626|\n",
      "|  TOSHIBA MQ01ABF050|     46969|      14|          10.879516|\n",
      "| TOSHIBA MQ01ABF050M|     32624|       3|           3.356425|\n",
      "|      WDC WD5000LPCX|      4920|       2|          14.837398|\n",
      "|      WDC WD5000LPVX|     22015|       2|           3.315921|\n",
      "|        WDC WD60EFRX|     30523|       1|           1.195820|\n",
      "+--------------------+----------+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating a table for failure rates that shows the annual failures rates for the drives\n",
    "failure_rates_df=spark.sql(\"SELECT drive_days.model AS model, drive_days.drive_days AS drive_days, failures.failures AS failures, 100.0 * (1.0 * failures) / (drive_days / 365.0) AS annual_failure_rate FROM drive_days, failures, model_count WHERE drive_days.model = failures.model AND model_count.model = failures.model ORDER BY model\").na.drop()\n",
    "failure_rates_df.createOrReplaceTempView(\"failure_rates\")\n",
    "failure_rates_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+-------------------+-----------+\n",
      "|               model|drive_days|failures|annual_failure_rate|        AFR|\n",
      "+--------------------+----------+--------+-------------------+-----------+\n",
      "|HGST HMS5C4040ALE640|    313383|       2|           0.232942| [0.232942]|\n",
      "|HGST HMS5C4040BLE640|   1172824|      11|           0.342336| [0.342336]|\n",
      "|HGST HUH721212ALE600|     14040|       1|           2.599715| [2.599715]|\n",
      "|HGST HUH721212ALN604|    259749|       4|           0.562081| [0.562081]|\n",
      "|HGST HUH728080ALE600|     93598|       3|           1.169897| [1.169897]|\n",
      "|       ST10000NM0086|    108555|       3|           1.008705| [1.008705]|\n",
      "|       ST12000NM0007|   2955025|     180|           2.223331| [2.223331]|\n",
      "|         ST4000DM000|   1989429|     107|           1.963126| [1.963126]|\n",
      "|       ST500LM012 HN|     50619|      12|           8.652877| [8.652877]|\n",
      "|          ST500LM030|     14479|       9|          22.688031|[22.688031]|\n",
      "|         ST6000DX000|    135832|       1|           0.268714| [0.268714]|\n",
      "|         ST8000DM002|    888741|      29|           1.191011| [1.191011]|\n",
      "|         ST8000DM004|       273|       1|         133.699670|[133.69967]|\n",
      "|        ST8000NM0055|   1294451|      58|           1.635442| [1.635442]|\n",
      "| TOSHIBA MG07ACA14TA|    109404|       1|           0.333626| [0.333626]|\n",
      "|  TOSHIBA MQ01ABF050|     46969|      14|          10.879516|[10.879516]|\n",
      "| TOSHIBA MQ01ABF050M|     32624|       3|           3.356425| [3.356425]|\n",
      "|      WDC WD5000LPCX|      4920|       2|          14.837398|[14.837398]|\n",
      "|      WDC WD5000LPVX|     22015|       2|           3.315921| [3.315921]|\n",
      "|        WDC WD60EFRX|     30523|       1|           1.195820|  [1.19582]|\n",
      "+--------------------+----------+--------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#making a vector from the dataframe given\n",
    "failure_vector= VectorAssembler(inputCols=[\"annual_failure_rate\"], outputCol=\"AFR\")\n",
    "failure_predict = failure_vector.transform(failure_rates_df)\n",
    "failure_predict.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the prediction using k means\n",
    "kmeans = KMeans().setK(4).setSeed(1).setFeaturesCol(\"AFR\")\n",
    "model = kmeans.fit(failure_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[1.42660613]\n",
      "[133.69967]\n",
      "[22.688031]\n",
      "[11.456597]\n"
     ]
    }
   ],
   "source": [
    "#finding and printing the cluster centres\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+-------------------+-----------+----------+\n",
      "|               model|drive_days|failures|annual_failure_rate|        AFR|prediction|\n",
      "+--------------------+----------+--------+-------------------+-----------+----------+\n",
      "|HGST HMS5C4040ALE640|    313383|       2|           0.232942| [0.232942]|         0|\n",
      "|         ST6000DX000|    135832|       1|           0.268714| [0.268714]|         0|\n",
      "| TOSHIBA MG07ACA14TA|    109404|       1|           0.333626| [0.333626]|         0|\n",
      "|HGST HMS5C4040BLE640|   1172824|      11|           0.342336| [0.342336]|         0|\n",
      "|HGST HUH721212ALN604|    259749|       4|           0.562081| [0.562081]|         0|\n",
      "|       ST10000NM0086|    108555|       3|           1.008705| [1.008705]|         0|\n",
      "|HGST HUH728080ALE600|     93598|       3|           1.169897| [1.169897]|         0|\n",
      "|         ST8000DM002|    888741|      29|           1.191011| [1.191011]|         0|\n",
      "|        WDC WD60EFRX|     30523|       1|           1.195820|  [1.19582]|         0|\n",
      "|        ST8000NM0055|   1294451|      58|           1.635442| [1.635442]|         0|\n",
      "|         ST4000DM000|   1989429|     107|           1.963126| [1.963126]|         0|\n",
      "|       ST12000NM0007|   2955025|     180|           2.223331| [2.223331]|         0|\n",
      "|HGST HUH721212ALE600|     14040|       1|           2.599715| [2.599715]|         0|\n",
      "|      WDC WD5000LPVX|     22015|       2|           3.315921| [3.315921]|         0|\n",
      "| TOSHIBA MQ01ABF050M|     32624|       3|           3.356425| [3.356425]|         0|\n",
      "|       ST500LM012 HN|     50619|      12|           8.652877| [8.652877]|         3|\n",
      "|  TOSHIBA MQ01ABF050|     46969|      14|          10.879516|[10.879516]|         3|\n",
      "|      WDC WD5000LPCX|      4920|       2|          14.837398|[14.837398]|         3|\n",
      "|          ST500LM030|     14479|       9|          22.688031|[22.688031]|         2|\n",
      "|         ST8000DM004|       273|       1|         133.699670|[133.69967]|         1|\n",
      "+--------------------+----------+--------+-------------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#displaying the results of the predicted clusters\n",
    "transformed_df = model.transform(failure_predict).orderBy(\"annual_failure_rate\")\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+-------------------+-----------+----------+----------+\n",
      "|               model|drive_days|failures|annual_failure_rate|        AFR|prediction|  distance|\n",
      "+--------------------+----------+--------+-------------------+-----------+----------+----------+\n",
      "|HGST HMS5C4040ALE640|    313383|       2|           0.232942| [0.232942]|         0| 1.1936641|\n",
      "|         ST6000DX000|    135832|       1|           0.268714| [0.268714]|         0| 1.1578921|\n",
      "| TOSHIBA MG07ACA14TA|    109404|       1|           0.333626| [0.333626]|         0| 1.0929801|\n",
      "|HGST HMS5C4040BLE640|   1172824|      11|           0.342336| [0.342336]|         0| 1.0842701|\n",
      "|HGST HUH721212ALN604|    259749|       4|           0.562081| [0.562081]|         0|0.86452514|\n",
      "|       ST10000NM0086|    108555|       3|           1.008705| [1.008705]|         0|0.41790113|\n",
      "|HGST HUH728080ALE600|     93598|       3|           1.169897| [1.169897]|         0|0.25670913|\n",
      "|         ST8000DM002|    888741|      29|           1.191011| [1.191011]|         0|0.23559514|\n",
      "|        WDC WD60EFRX|     30523|       1|           1.195820|  [1.19582]|         0|0.23078613|\n",
      "|        ST8000NM0055|   1294451|      58|           1.635442| [1.635442]|         0|0.20883587|\n",
      "|         ST4000DM000|   1989429|     107|           1.963126| [1.963126]|         0| 0.5365199|\n",
      "|       ST12000NM0007|   2955025|     180|           2.223331| [2.223331]|         0|0.79672486|\n",
      "|HGST HUH721212ALE600|     14040|       1|           2.599715| [2.599715]|         0| 1.1731088|\n",
      "|      WDC WD5000LPVX|     22015|       2|           3.315921| [3.315921]|         0| 1.8893149|\n",
      "| TOSHIBA MQ01ABF050M|     32624|       3|           3.356425| [3.356425]|         0| 1.9298189|\n",
      "|       ST500LM012 HN|     50619|      12|           8.652877| [8.652877]|         3|   2.80372|\n",
      "|  TOSHIBA MQ01ABF050|     46969|      14|          10.879516|[10.879516]|         3|  0.577081|\n",
      "|      WDC WD5000LPCX|      4920|       2|          14.837398|[14.837398]|         3|  3.380801|\n",
      "|          ST500LM030|     14479|       9|          22.688031|[22.688031]|         2|       0.0|\n",
      "|         ST8000DM004|       273|       1|         133.699670|[133.69967]|         1|       0.0|\n",
      "+--------------------+----------+--------+-------------------+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculating the distance from the cluster centers\n",
    "distance_udf = udf(lambda x,y: float(distance.euclidean(x, centers[y])), FloatType())\n",
    "transformed_df = transformed_df.withColumn(\"distance\", distance_udf(col(\"AFR\"),col(\"prediction\")))\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+--------+-------------------+----------+----------+---------+\n",
      "|              model|drive_days|failures|annual_failure_rate|       AFR|prediction| distance|\n",
      "+-------------------+----------+--------+-------------------+----------+----------+---------+\n",
      "|     WDC WD5000LPVX|     22015|       2|           3.315921|[3.315921]|         0|1.8893149|\n",
      "|TOSHIBA MQ01ABF050M|     32624|       3|           3.356425|[3.356425]|         0|1.9298189|\n",
      "+-------------------+----------+--------+-------------------+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#shows that all the values that are over the threshold. (outliars)\n",
    "filter_udf = udf(lambda x: float(centers[x]+0.02*centers[x]), FloatType())\n",
    "transformed_df.filter(col(\"distance\")>=filter_udf(col(\"prediction\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Question 3(b):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|smart_1_normalized|\n",
      "+------------------+\n",
      "|             117.0|\n",
      "|              80.0|\n",
      "|              83.0|\n",
      "|              81.0|\n",
      "|             100.0|\n",
      "|              75.0|\n",
      "|              83.0|\n",
      "|              83.0|\n",
      "|              78.0|\n",
      "|              77.0|\n",
      "|             117.0|\n",
      "|              81.0|\n",
      "|              74.0|\n",
      "|              80.0|\n",
      "|              78.0|\n",
      "|             100.0|\n",
      "|             100.0|\n",
      "|             100.0|\n",
      "|             100.0|\n",
      "|             100.0|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#making the table with the values \n",
    "smart_df=pre_final.select(\"smart_1_normalized\").na.drop()\n",
    "smart_df=smart_df.withColumn(\"smart_1_normalized\",smart_df[\"smart_1_normalized\"].cast(\"float\"))\n",
    "smart_df.createOrReplaceTempView(\"smart\")\n",
    "smart_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "|smart_1_normalized|smart_value|\n",
      "+------------------+-----------+\n",
      "|             117.0|    [117.0]|\n",
      "|              80.0|     [80.0]|\n",
      "|              83.0|     [83.0]|\n",
      "|              81.0|     [81.0]|\n",
      "|             100.0|    [100.0]|\n",
      "|              75.0|     [75.0]|\n",
      "|              83.0|     [83.0]|\n",
      "|              83.0|     [83.0]|\n",
      "|              78.0|     [78.0]|\n",
      "|              77.0|     [77.0]|\n",
      "|             117.0|    [117.0]|\n",
      "|              81.0|     [81.0]|\n",
      "|              74.0|     [74.0]|\n",
      "|              80.0|     [80.0]|\n",
      "|              78.0|     [78.0]|\n",
      "|             100.0|    [100.0]|\n",
      "|             100.0|    [100.0]|\n",
      "|             100.0|    [100.0]|\n",
      "|             100.0|    [100.0]|\n",
      "|             100.0|    [100.0]|\n",
      "+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Vectorized values of the column that has to be evaluated \n",
    "smart_vector= VectorAssembler(inputCols=[\"smart_1_normalized\"], outputCol=\"smart_value\")\n",
    "smart_predict = smart_vector.transform(smart_df)\n",
    "smart_predict.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction by clustering algorithm k-means\n",
    "kmeans_3b = KMeans().setK(4).setSeed(1).setFeaturesCol(\"smart_value\")\n",
    "model_3b = kmeans_3b.fit(smart_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[79.80602865]\n",
      "[100.38512109]\n",
      "[116.09234413]\n",
      "[199.99904928]\n"
     ]
    }
   ],
   "source": [
    "#finding and printing the cluster centres\n",
    "centers_3b = model_3b.clusterCenters()\n",
    "centers_3b.sort()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers_3b:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+----------+\n",
      "|smart_1_normalized|smart_value|prediction|\n",
      "+------------------+-----------+----------+\n",
      "|             117.0|    [117.0]|         2|\n",
      "|              80.0|     [80.0]|         1|\n",
      "|              83.0|     [83.0]|         1|\n",
      "|              81.0|     [81.0]|         1|\n",
      "|             100.0|    [100.0]|         0|\n",
      "|              75.0|     [75.0]|         1|\n",
      "|              83.0|     [83.0]|         1|\n",
      "|              83.0|     [83.0]|         1|\n",
      "|              78.0|     [78.0]|         1|\n",
      "|              77.0|     [77.0]|         1|\n",
      "|             117.0|    [117.0]|         2|\n",
      "|              81.0|     [81.0]|         1|\n",
      "|              74.0|     [74.0]|         1|\n",
      "|              80.0|     [80.0]|         1|\n",
      "|              78.0|     [78.0]|         1|\n",
      "|             100.0|    [100.0]|         0|\n",
      "|             100.0|    [100.0]|         0|\n",
      "|             100.0|    [100.0]|         0|\n",
      "|             100.0|    [100.0]|         0|\n",
      "|             100.0|    [100.0]|         0|\n",
      "+------------------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#showing the predicted cluster values\n",
    "smart_transformed = model_3b.transform(smart_predict)\n",
    "smart_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+----------+---------+\n",
      "|smart_1_normalized|smart_value|prediction| distance|\n",
      "+------------------+-----------+----------+---------+\n",
      "|             117.0|    [117.0]|         2|0.9076559|\n",
      "|              80.0|     [80.0]|         1| 20.38512|\n",
      "|              83.0|     [83.0]|         1| 17.38512|\n",
      "|              81.0|     [81.0]|         1| 19.38512|\n",
      "|             100.0|    [100.0]|         0|20.193972|\n",
      "|              75.0|     [75.0]|         1| 25.38512|\n",
      "|              83.0|     [83.0]|         1| 17.38512|\n",
      "|              83.0|     [83.0]|         1| 17.38512|\n",
      "|              78.0|     [78.0]|         1| 22.38512|\n",
      "|              77.0|     [77.0]|         1| 23.38512|\n",
      "|             117.0|    [117.0]|         2|0.9076559|\n",
      "|              81.0|     [81.0]|         1| 19.38512|\n",
      "|              74.0|     [74.0]|         1| 26.38512|\n",
      "|              80.0|     [80.0]|         1| 20.38512|\n",
      "|              78.0|     [78.0]|         1| 22.38512|\n",
      "|             100.0|    [100.0]|         0|20.193972|\n",
      "|             100.0|    [100.0]|         0|20.193972|\n",
      "|             100.0|    [100.0]|         0|20.193972|\n",
      "|             100.0|    [100.0]|         0|20.193972|\n",
      "|             100.0|    [100.0]|         0|20.193972|\n",
      "+------------------+-----------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#finding the distances from the cluster centers\n",
    "distance_udf = udf(lambda x,y: float(distance.euclidean(x, centers_3b[y])), FloatType())\n",
    "smart_transformed = smart_transformed.withColumn(\"distance\", distance_udf(col(\"smart_value\"),col(\"prediction\")))\n",
    "smart_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows that all the values that are over the threshold. (outliars)\n",
    "filter_udf = udf(lambda x: float(centers_3b[x]+100.0), FloatType())\n",
    "smart_transformed = smart_transformed.filter(col(\"distance\")>=filter_udf(col(\"prediction\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o282.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 194.0 failed 1 times, most recent failure: Lost task 1.0 in stage 194.0 (TID 5529, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 362, in main\n  File \"C:\\spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 722, in read_int\n    length = stream.read(4)\n  File \"C:\\Users\\myste\\AppData\\Local\\Programs\\Python\\Python37\\lib\\socket.py\", line 589, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 362, in main\n  File \"C:\\spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 722, in read_int\n    length = stream.read(4)\n  File \"C:\\Users\\myste\\AppData\\Local\\Programs\\Python\\Python37\\lib\\socket.py\", line 589, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-4aa9cc218b3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msmart_transformed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \"\"\"\n\u001b[0;32m    379\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o282.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 194.0 failed 1 times, most recent failure: Lost task 1.0 in stage 194.0 (TID 5529, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 362, in main\n  File \"C:\\spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 722, in read_int\n    length = stream.read(4)\n  File \"C:\\Users\\myste\\AppData\\Local\\Programs\\Python\\Python37\\lib\\socket.py\", line 589, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 362, in main\n  File \"C:\\spark\\spark-2.4.5-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 722, in read_int\n    length = stream.read(4)\n  File \"C:\\Users\\myste\\AppData\\Local\\Programs\\Python\\Python37\\lib\\socket.py\", line 589, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "'''The filter function is running but the show function is showing an error due to memory \n",
    "problems in jupyter and databricks.com. But the logic is same as 3a which seems to be correct.'''\n",
    "smart_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
